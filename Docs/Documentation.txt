Step 1: Define the Data Model for Intent
    I basically need a well-defined, validated structure for what the LLM will return.
    This ensures that the rest of our application can reliably work with the LLM's output.
    I'll use Pydantic for this, which enforces data types and validation. 

    Used classes as I'm making sure that I follow SOLID principles.

Step 2: Define the Abstract LLM interface
    I'll define the contract for any LLM parser we might use. This abstraction allows us to
    easily swap out OpenRouter for another service (maybe OpenAI or local LLM(If I care enought to develop one!))
    in the future without changing the rest of the application.
    (Again I looked forward to adhere to the SOLID). 

Step 3: Implement the OpenRouter LLM parser
    Now, I'll create the concrete implementation that uses OpenRouter's API to parse the command.
    This class will inherit from LLMInterface and implement the `parse_command` method.

    I'll use the requests library for the API call and python-dotenv to manage the API key securely.

====================================================================================================================

## Run both on the parallel terminals

uvicorn api.main:app --reload

cd frontend
npm start

====================================================================================================================

"Failed to fetch" error is a classic CORS (Cross-Origin Resource Sharing) issue.
    Context: upon uploading the .csv, 'Failed to fetch' appears.
    
====================================================================================================================

To improve the prompt template and the accuracy of the output provided by the llm:
    We have to keep on improving P.A.N.D-A/app/processing/pandas_processor.py

====================================================================================================================

Now let's make the UI better as we have got a working model of our project.

- Providing a real-time log stream gives the user transparency into the backend processes, 
which is fantastic for building trust and for debugging.
- Directly showing the terminal output is not possible due to browser security restrictions. 
However, we can achieve the exact same effect by implementing a modern, real-time communication channel between our backend and frontend using WebSockets.

Here's the plan:
    - Backend (FastAPI): We will create a WebSocket endpoint (/ws/logs). We'll then redirect all of our application's 
    logging output to be broadcast over this WebSocket to any connected clients.

    - Frontend (React): We will create a new TerminalView component. This component will connect to 
    the WebSocket endpoint, listen for log messages, and display them in a terminal-like window. A button in the main App will toggle its visibility.

Step 1: Modify the Backend to broadcast logs

====================================================================================================================

Now I'll add a choice - the user can upload his/her own .csv file or can use my own coffee.csv for testing and experience.

Also added previewing of the data file selected (only the top 5 row, for now)

====================================================================================================================

We'll add support for plotting charts, basically visual representation.
I'll use: chart.js and react-chartjs-2

====================================================================================================================

Intent and pandas_processor has to be updated to support the new charting functionality.
Currently, the template is designed to handle basic data processing and visualization.

None of those 3 commands work!